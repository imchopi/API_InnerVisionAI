## 1. JustificaciÃ³n y descripciÃ³n del proyecto.

## 2. ObtenciÃ³n de datos. Se debe especificar la fuente de los datos. Se indicarÃ¡ por quÃ© medios se han obtenido (encuestas, sensores, scrapping, etc.). Los datos se deben cargar en una estructura que permita su posterior manipulaciÃ³n y uso.

Para conseguir los datos que necesitÃ¡bamos, decidimos extraer imÃ¡genes de internet ğŸ–¼ï¸. Como nuestro modelo va a funcionar en tiempo real â³, querÃ­amos que reconociera objetos comunes en la empresa, como sillas, armarios y mesas ğŸª‘ğŸ“¦.

Por eso elegimos IKEA ğŸ  como fuente de informaciÃ³n, ya que es una de las mejores referencias y sabemos que tiene justo lo que estÃ¡bamos buscando âœ….

Lo primero que hicimos fue instalar las librerÃ­as necesarias ğŸ› ï¸. Al principio, como la web parecÃ­a sencilla ğŸŒ, pensamos que con BeautifulSoup ğŸ¥£ serÃ­a suficiente, pero no fue asÃ­ âŒ.

Si querÃ­amos cargar todas las imÃ¡genes ğŸ–¼ï¸, nos dimos cuenta de que habÃ­a un botÃ³n de "Mostrar mÃ¡s", que era el encargado de cargar las imÃ¡genes.

Para solucionar este problema, utilizamos Selenium ğŸš—ğŸ’¨, que nos permite interactuar con la web como si fuÃ©ramos un usuario real.

A continuaciÃ³n, explicarÃ© las librerÃ­as que usaremos ğŸ“š para llevar a cabo la extracciÃ³n de datos ğŸ”.

### ğŸ“Œ LibrerÃ­as utilizadas para la extracciÃ³n de datos

# Foto 

Para poder extraer informaciÃ³n de la web, utilizamos varias librerÃ­as ğŸ“š que nos ayudarÃ¡n en diferentes tareas:

os ğŸ“‚ â†’ Nos permite interactuar con los archivos y carpetas de nuestro sistema.

time â³ â†’ Se usa para pausar el cÃ³digo y esperar antes de realizar ciertas acciones.

requests ğŸŒ â†’ Nos ayuda a hacer peticiones a sitios web y obtener su contenido.

re ğŸ” â†’ Se usa para trabajar con patrones de texto, como encontrar enlaces o filtrar datos.
BeautifulSoup (de bs4) ğŸ¥£ â†’ Nos permite extraer informaciÃ³n de una pÃ¡gina web de forma mÃ¡s sencilla.

ğŸš€ AutomatizaciÃ³n con Selenium

webdriver ğŸš— â†’ Nos permite controlar un navegador web (como Chrome).

Service ğŸ› ï¸ â†’ Maneja la configuraciÃ³n del navegador para Selenium.

Options âš™ï¸ â†’ Nos ayuda a establecer preferencias para el navegador (como abrirlo en modo 
invisible).

By ğŸ” â†’ Se usa para encontrar elementos dentro de la pÃ¡gina web.

WebDriverWait â³ â†’ Permite que el cÃ³digo espere hasta que un elemento de la web aparezca.

expected_conditions (EC) âœ… â†’ Nos ayuda a esperar hasta que ciertas condiciones se cumplan, como que un botÃ³n sea visible antes de hacer clic.

## ğŸ” Estructura del CÃ³digo  

Lo siguiente que mostrarÃ© es un poco cÃ³mo he **organizado** este cÃ³digo ğŸ“œ.  

Voy a explicar **cada parte** para que sea fÃ¡cil de entender y seguir ğŸ“Œ.  

---

## FOTO 

# ğŸ–¥ï¸ Clase IKEA Scraper

Esta clase se encarga de **extraer informaciÃ³n** de la web de **IKEA** ğŸ , especÃ­ficamente imÃ¡genes de **armarios, mesas y sillas**.  

## ğŸ“Œ 1. InicializaciÃ³n (`__init__` method)  
Cuando creamos un objeto de esta clase, se ejecuta este cÃ³digo automÃ¡ticamente.  

### ğŸ”— `self.category_urls`  
- Es un diccionario que contiene las **categorÃ­as de muebles** y los enlaces de IKEA donde estÃ¡n los productos ğŸ“¦.  
- Por ejemplo, para obtener **armarios**, el cÃ³digo buscarÃ¡ en:  
  - `"https://www.ikea.com/es/es/cat/armarios-19053/"`  

### ğŸ·ï¸ `self.headers`  
- Sirve para **simular** que el navegador es un usuario real y evitar bloqueos de la web.  
- AquÃ­ se usa un **"User-Agent"** (que le dice a la web que estamos usando un navegador como Chrome o Firefox).  

---

## ğŸš€ 2. ConfiguraciÃ³n del navegador con Selenium  
Como la web tiene **botones interactivos**, usamos **Selenium** para controlarla automÃ¡ticamente.  

### âš™ï¸ `chrome_options = Options()`  
- Nos permite **configurar** cÃ³mo se abrirÃ¡ el navegador.  
- `chrome_options.add_argument("--start-maximized")` ğŸ–¥ï¸  
  - Abre el navegador en **pantalla completa** para evitar problemas con elementos ocultos.  

### ğŸš— `self.driver = webdriver.Chrome(...)`  
- AquÃ­ estamos **iniciando Chrome** para que Selenium pueda interactuar con la web **como si fuera un usuario real**.  

---

## ğŸ“ FunciÃ³n `create_directories`

Esta funciÃ³n se encarga de **crear carpetas** en el sistema para almacenar las imÃ¡genes de cada categorÃ­a ğŸ—‚ï¸.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Obtiene las categorÃ­as** ğŸ·ï¸  
   - Usa `self.category_urls.keys()` para obtener la lista de categorÃ­as disponibles (ejemplo: "sillas", "mesas", "armarios").  

2. **Crea una carpeta para cada categorÃ­a** ğŸ“‚  
   - Recorre la lista de categorÃ­as y construye la ruta donde se guardarÃ¡n los datos.  
   - Usa `os.path.join(base_path, category)` para combinar la ruta base con el nombre de la categorÃ­a.  

3. **Verifica si la carpeta existe** âœ…  
   - Si la carpeta **no existe**, se crea con `os.makedirs(path)`.  

4. **Devuelve la lista de categorÃ­as** ğŸ”„  
   - Retorna la lista de categorÃ­as que se crearon.  

--- 

## FOTO 

## ğŸ–¼ï¸ FunciÃ³n `download_image`

Esta funciÃ³n **descarga una imagen desde una URL y la guarda en el sistema** ğŸ“¥ğŸ“‚.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Hace una solicitud a la URL** ğŸŒ  
   - Utiliza `requests.get(url, headers=self.headers)` para obtener la imagen desde el enlace.  
   - Usa **`self.headers`** para simular una peticiÃ³n desde un navegador y evitar bloqueos.  

2. **Verifica que la imagen se haya descargado correctamente** âœ…  
   - Si el cÃ³digo de respuesta (`response.status_code`) es `200`, significa que la imagen se descargÃ³ sin problemas.  

3. **Guarda la imagen en el archivo especificado** ğŸ’¾  
   - Abre el archivo en modo escritura binaria (`'wb'`).  
   - Escribe el contenido de la imagen en el archivo.  
   - Imprime un mensaje confirmando que la imagen se guardÃ³ correctamente.  

4. **Manejo de errores** âš ï¸  
   - Si ocurre un error en la descarga, se captura con `except Exception as e`.  
   - Se imprime un mensaje indicando el problema y la funciÃ³n devuelve `False`.  

---

# Foto 

## ğŸ”„ FunciÃ³n `load_all_products`

Esta funciÃ³n **carga todos los productos de la pÃ¡gina web** haciendo clic en el botÃ³n `"Mostrar mÃ¡s"` hasta que ya no haya mÃ¡s productos disponibles ğŸ›ï¸.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Abre la pÃ¡gina web** ğŸŒ  
   - Utiliza `self.driver.get(url)` para acceder a la URL proporcionada.  

2. **Espera a que la pÃ¡gina cargue** â³  
   - Crea una espera con `WebDriverWait` para asegurarse de que los elementos estÃ©n listos antes de interactuar con ellos.  

3. **Bucle para cargar mÃ¡s productos** ğŸ”„  
   - Se usa un `while True` para hacer clic en `"Mostrar mÃ¡s"` varias veces.  
   - **Busca el botÃ³n** con `wait.until(...)` y verifica que sea **clickeable**.  
   - **Ejecuta un clic** en el botÃ³n con `execute_script("arguments[0].click();", load_more_button)`.  
   - Muestra el mensaje `"Cargando mÃ¡s productos..."` en la terminal.  
   - Espera `2 segundos` (`time.sleep(2)`) antes de intentar hacer clic nuevamente.  

4. **Manejo de errores** âš ï¸  
   - Si el botÃ³n `"Mostrar mÃ¡s"` ya no estÃ¡ disponible, significa que **no hay mÃ¡s productos**.  
   - En ese caso, se muestra el mensaje `"No hay mÃ¡s productos para cargar."` y el bucle se detiene con `break`.

---

# Foto 

## ğŸ” FunciÃ³n `scrape_category`

Esta funciÃ³n **extrae imÃ¡genes de productos** de una **categorÃ­a especÃ­fica** en IKEA ğŸ  y las guarda en una carpeta ğŸ“‚.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Carga la pÃ¡gina de la categorÃ­a** ğŸŒ  
   - Obtiene la URL de la categorÃ­a desde `self.category_urls[category]`.  
   - Llama a `self.load_all_products(url)`, que se encarga de hacer clic en `"Mostrar mÃ¡s"` hasta que todos los productos estÃ©n cargados.  

2. **Extrae los productos con `BeautifulSoup`** ğŸ¥£  
   - Obtiene el cÃ³digo HTML de la pÃ¡gina con `self.driver.page_source`.  
   - Busca todos los productos en la web usando `soup.find_all('div', class_='plp-fragment-wrapper')`.  

3. **Recorre cada producto y extrae su informaciÃ³n** ğŸ”„  
   - Para cada producto, intenta encontrar:  
     - La **imagen** (`<img>`) y su URL.  
     - El **nombre del producto** dentro de un `<span>`.  

4. **Limpia el nombre del producto** ğŸ·ï¸  
   - Si el nombre contiene caracteres especiales, los reemplaza con `_` usando `re.sub()`.  
   - Si no se encuentra un nombre, usa un nombre genÃ©rico basado en la categorÃ­a y el Ã­ndice (`category_idx`).  

5. **Descarga la imagen** ğŸ“¥  
   - Guarda la imagen en la ruta `save_path`, nombrÃ¡ndola con el nombre limpio.  
   - Llama a `self.download_image(img_url, image_path)`, que descarga y guarda la imagen.  
   - Muestra un mensaje en la terminal confirmando la descarga.  

6. **Manejo de errores** âš ï¸  
   - Si hay un problema al procesar un producto, muestra un mensaje de error y pasa al siguiente.  

---

# Foto 

## ğŸš€ FunciÃ³n `run`

Esta funciÃ³n **ejecuta el scraping completo** en todas las categorÃ­as definidas ğŸ ğŸ”„.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Muestra un mensaje de inicio** ğŸ“¢  
   - Se imprime `"Iniciando proceso de scraping..."` para indicar que el proceso ha comenzado.  

2. **Crea los directorios para cada categorÃ­a** ğŸ“‚  
   - Llama a `self.create_directories(base_path)`, que se encarga de crear carpetas donde se guardarÃ¡n las imÃ¡genes.  
   - Muestra en la terminal las categorÃ­as para las cuales se han creado carpetas.  

3. **Realiza el scraping de cada categorÃ­a** ğŸ”„  
   - **Recorre todas las categorÃ­as disponibles** usando un `for`.  
   - **Imprime un mensaje indicando que estÃ¡ iniciando el scraping** de esa categorÃ­a.  
   - Genera la ruta donde se guardarÃ¡n las imÃ¡genes (`category_path`).  
   - Llama a `self.scrape_category(category, category_path)`, que extrae las imÃ¡genes de la web.  
   - Muestra un mensaje indicando que el scraping de esa categorÃ­a ha finalizado.  
   - **Espera 3 segundos (`time.sleep(3)`)** antes de pasar a la siguiente categorÃ­a.  

4. **Cierra el navegador** ğŸ”š  
   - Llama a `self.driver.quit()`, lo que finaliza el uso de Selenium y cierra el navegador.  

--- 

# Foto 

## ğŸ EjecuciÃ³n del scraper en local

Este bloque de cÃ³digo **inicia el proceso de scraping** cuando ejecutamos el script en nuestro ordenador ğŸ–¥ï¸.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Verifica si el script se estÃ¡ ejecutando directamente** ğŸ—ï¸  
   - La condiciÃ³n `if __name__ == "__main__":` se asegura de que el cÃ³digo **solo se ejecute** cuando ejecutamos el archivo directamente y no si se importa como mÃ³dulo en otro script.  

2. **Crea una instancia del scraper** ğŸ   
   - Se inicializa un objeto `IKEAScraper`, indicando la ruta del **ChromeDriver** (`chromedriver.exe`).  
   - `ChromeDriver` es necesario para que **Selenium** pueda controlar el navegador.  

3. **Ejecuta el proceso completo de scraping** ğŸš€  
   - Se llama a `scraper.run()`, que se encargarÃ¡ de:
     - Abrir el navegador.
     - Extraer los productos de todas las categorÃ­as.
     - Descargar las imÃ¡genes.
     - Guardarlas en carpetas organizadas.

---

## ğŸ–¥ï¸ Ampliando el scraping: objetos mÃ¡s pequeÃ±os  

DespuÃ©s de extraer informaciÃ³n sobre **muebles**, decidimos tambiÃ©n obtener datos de **objetos mÃ¡s pequeÃ±os** que se pueden encontrar dentro de la empresa, como **teclados, portÃ¡tiles y ratones** âŒ¨ï¸ğŸ’»ğŸ–±ï¸.  

### ğŸ” Cambio de fuente: PCComponentes  
Para estos productos, optamos por **extraer la informaciÃ³n de PCComponentes** ğŸ›’.  

Sin embargo, pronto nos dimos cuenta de una **diferencia clave** con la web de IKEA:  
ğŸ”¹ **No tenÃ­a un botÃ³n de "Cargar mÃ¡s"** como IKEA.  
ğŸ”¹ En su lugar, habÃ­a un **botÃ³n para pasar a la siguiente pÃ¡gina** ğŸ“„â¡ï¸.  

### ğŸš§ Problema encontrado: Bloqueo de bots  
Cuando intentamos **pasar a la siguiente pÃ¡gina**, la web nos detectaba como **un bot** ğŸš«ğŸ¤–, impidiendo que siguiÃ©ramos navegando.  

### ğŸ’¡ SoluciÃ³n ingeniosa  
Para **evitar el bloqueo**, encontramos una soluciÃ³n **sencilla pero efectiva**:  

ğŸ”„ En lugar de hacer clic en **"Siguiente pÃ¡gina"**, usamos un **bucle** que:  
1. **Scrapea** la informaciÃ³n de la pÃ¡gina actual.  
2. **Cierra el navegador** completamente.  
3. **Vuelve a abrirlo** en la siguiente pÃ¡gina con la nueva URL.  

ğŸ“Œ **Resultado:** La web no detectaba que Ã©ramos un bot, permitiÃ©ndonos extraer la informaciÃ³n sin problemas ğŸ¯.  

---

## ğŸ–¥ï¸ CÃ³digo para extraer datos de PCComponentes  

En esta situaciÃ³n, utilizaremos **tres cÃ³digos similares** ğŸ”„, pero con algunos **detalles especÃ­ficos** cambiados para que cada uno **interactÃºe con su categorÃ­a correspondiente**.  

ğŸ“Œ **Las tres categorÃ­as a scrapear son:**  
- **Teclados** âŒ¨ï¸  
- **PortÃ¡tiles** ğŸ’»  
- **Ratones** ğŸ–±ï¸  

Dado que los cÃ³digos son casi **idÃ©nticos**, solo mostrarÃ© el cÃ³digo correspondiente a **los ratones** ğŸ–±ï¸, ya que la lÃ³gica es la misma para los otros dos casos, con pequeÃ±os ajustes.  

---

ğŸ“Œ **A continuaciÃ³n, el cÃ³digo para extraer informaciÃ³n de los ratones en PCComponentes** ğŸš€.

# Foto 

## ğŸ“š LibrerÃ­as utilizadas para el scraping de PCComponentes  

Para poder extraer los datos de **PCComponentes**, utilizamos varias **librerÃ­as** ğŸ“¦ que nos ayudarÃ¡n en diferentes tareas:  

### ğŸ”¹ **LibrerÃ­as bÃ¡sicas**
- **`requests`** ğŸŒ â†’ Se usa para hacer peticiones a la web y obtener su contenido.  
- **`os`** ğŸ“‚ â†’ Permite interactuar con los archivos y carpetas del sistema.  
- **`time`** â³ â†’ Se usa para hacer pausas en el cÃ³digo y evitar bloqueos por sobrecarga de peticiones.  

### ğŸ¥£ **BeautifulSoup: AnÃ¡lisis del HTML**
- **`BeautifulSoup (bs4)`** ğŸ—ï¸ â†’ Se encarga de **extraer informaciÃ³n del cÃ³digo HTML** de la web.  

### ğŸš€ **Selenium: AutomatizaciÃ³n del navegador**  
Dado que la web de **PCComponentes** requiere **interacciÃ³n con la paginaciÃ³n**, usamos **Selenium** para controlarla:  

- **`webdriver`** ğŸš— â†’ Controla el navegador (Chrome en este caso).  
- **`Service`** ğŸ› ï¸ â†’ Configura la ejecuciÃ³n del navegador de forma automÃ¡tica.  
- **`Options`** âš™ï¸ â†’ Permite configurar cÃ³mo se abre el navegador (ejemplo: en modo sin interfaz).  
- **`By`** ğŸ” â†’ Se usa para buscar elementos dentro de la web (ejemplo: botones o imÃ¡genes).  
- **`WebDriverWait`** â³ â†’ Permite que el cÃ³digo **espere** hasta que un elemento de la web se cargue correctamente.  
- **`expected_conditions (EC)`** âœ… â†’ Se usa para definir condiciones antes de interactuar con elementos de la web (como esperar a que un botÃ³n sea clickeable).  

---

# FOTO

## ğŸ–±ï¸ Clase `PCscrapper`

Esta clase se encarga de **extraer informaciÃ³n de los ratones en PCComponentes** ğŸ›’.  

### ğŸ› ï¸ **InicializaciÃ³n de la clase (`__init__` method)**  

Cuando creamos un objeto de esta clase, se ejecuta el cÃ³digo de inicializaciÃ³n que:  

### ğŸ”— **Define las URLs de las pÃ¡ginas a scrapear**  
- **`self.category_urls`** almacena un **diccionario de URLs**, donde cada clave es el nombre de la pÃ¡gina y el valor es la URL correspondiente.  
- Se usa un **bucle con `range(1, 29)`**, lo que indica que se van a recorrer **28 pÃ¡ginas** (de la 1 a la 28).  
- La URL cambia dinÃ¡micamente con `?page={page}`, lo que permite navegar por las diferentes pÃ¡ginas sin hacer clic en "Siguiente".  

### ğŸ·ï¸ **Define las cabeceras (`headers`)**  
- **`self.headers`** almacena un **"User-Agent"**, que hace que la peticiÃ³n parezca de un navegador real.  
- Esto ayuda a **evitar bloqueos** por parte de la web al detectar actividad sospechosa de bots ğŸ¤–ğŸš«.  

---

# Foto 

## ğŸ“‚ FunciÃ³n `create_directories`

Esta funciÃ³n se encarga de **crear carpetas** en el sistema para almacenar las imÃ¡genes de cada categorÃ­a ğŸ—‚ï¸.

### ğŸ› ï¸ Â¿CÃ³mo funciona?

1. **Obtiene las categorÃ­as** ğŸ·ï¸  
   - Usa `self.category_urls.keys()` para obtener la lista de categorÃ­as disponibles (ejemplo: `"ratones1"`, `"ratones2"`, `"ratones3"`, etc.).  

2. **Crea una carpeta para cada categorÃ­a** ğŸ“‚  
   - Recorre la lista de categorÃ­as y **construye la ruta** donde se guardarÃ¡n los datos.  
   - Usa `os.path.join(base_path, category)` para combinar la ruta base con el nombre de la categorÃ­a.  

3. **Verifica si la carpeta existe** âœ…  
   - Si la carpeta **no existe**, se crea con `os.makedirs(path)`.  

4. **Devuelve la lista de categorÃ­as** ğŸ”„  
   - Retorna la lista de categorÃ­as que se crearon.  

---

# Foto 

## ğŸ–¼ï¸ FunciÃ³n `download_image`

Esta funciÃ³n **descarga una imagen desde una URL y la guarda en el sistema** ğŸ“¥ğŸ“‚.

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**

1. **Hace una solicitud a la URL** ğŸŒ  
   - Usa `requests.get(url, headers=self.headers)` para obtener la imagen desde el enlace.  
   - Incluye **cabeceras HTTP (`headers`)** para **simular una peticiÃ³n real de un navegador** y evitar bloqueos.  

2. **Verifica que la imagen se haya descargado correctamente** âœ…  
   - Si el cÃ³digo de respuesta (`response.status_code`) es **200**, significa que la imagen se descargÃ³ sin problemas.  

3. **Crea el directorio si no existe** ğŸ“‚  
   - Usa `os.makedirs(os.path.dirname(path), exist_ok=True)` para asegurarse de que la carpeta donde se guardarÃ¡ la imagen **exista**.  
   - `exist_ok=True` evita errores si la carpeta ya estÃ¡ creada.  

4. **Guarda la imagen en el archivo especificado** ğŸ’¾  
   - Abre el archivo en modo escritura binaria (`'wb'`).  
   - Escribe el contenido de la imagen en el archivo.  
   - Imprime un mensaje confirmando que la imagen se guardÃ³ correctamente.  

5. **Manejo de errores** âš ï¸  
   - Si ocurre un error en la descarga, se captura con `except Exception as e`.  
   - Se imprime un mensaje indicando el problema y la funciÃ³n devuelve `False`.  

---

# Foto 

## ğŸ“¸ FunciÃ³n `download_products_images`

Esta funciÃ³n **descarga imÃ¡genes de una pÃ¡gina web de manera automÃ¡tica** ğŸŒğŸ“¥.

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**

1. **Abre la pÃ¡gina web en Chrome** ğŸŒ  
   - Usa **Selenium** para abrir la URL en un navegador.  
   - Configura el navegador en **modo maximizado** para evitar problemas de visualizaciÃ³n.  

2. **Acepta las cookies si es necesario** ğŸª  
   - Busca el botÃ³n de "Aceptar cookies" y hace clic en Ã©l.  
   - Si el botÃ³n no estÃ¡ presente, **el cÃ³digo sigue sin problemas**.  

3. **Espera a que las imÃ¡genes se carguen** â³  
   - Usa `WebDriverWait` para asegurarse de que las imÃ¡genes estÃ©n visibles antes de continuar.  
   - Utiliza **BeautifulSoup** para analizar el cÃ³digo HTML de la pÃ¡gina y extraer las imÃ¡genes.  

4. **Busca y filtra imÃ¡genes relevantes** ğŸ”  
   - Extrae todas las etiquetas `<img>` que tengan atributos `src` (enlace de imagen) y `alt` (descripciÃ³n).  
   - Obtiene la URL de cada imagen.  
   - Si la URL es **relativa** (empieza con `/`), se convierte en una URL **completa** agregando el dominio.  

5. **Descarga y guarda las imÃ¡genes** ğŸ“‚  
   - Genera un **nombre de archivo Ãºnico** (`image_{i}.jpg`) y lo guarda en una carpeta segÃºn su categorÃ­a.  
   - Llama a la funciÃ³n `self.download_image()` para almacenar la imagen correctamente.  

6. **Manejo de errores y cierre del navegador** âš ï¸  
   - Si hay un error durante la ejecuciÃ³n, se muestra un mensaje con la URL afectada.  
   - Al final, **siempre se cierra el navegador** para liberar recursos.  

---

# Foto 

## ğŸš€ FunciÃ³n `run`

Esta funciÃ³n **ejecuta el scraper** y se encarga de **organizar y descargar las imÃ¡genes** de diferentes categorÃ­as ğŸ“¥ğŸ“‚.

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**

1. **Define la carpeta base donde se guardarÃ¡n las imÃ¡genes** ğŸ“‚  
   - `base_path = 'images'` establece el directorio principal donde se almacenarÃ¡n las imÃ¡genes descargadas.  

2. **Crea las carpetas necesarias** ğŸ—ï¸  
   - Usa `self.create_directories(base_path)` para asegurarse de que la carpeta base **exista antes de guardar las imÃ¡genes**.  
   - Si la carpeta ya existe, **evita errores y continÃºa con la ejecuciÃ³n**.  

3. **Recorre todas las categorÃ­as de productos** ğŸ·ï¸  
   - Usa un **bucle `for`** para iterar sobre `self.category_urls.items()`, donde cada categorÃ­a tiene una URL especÃ­fica.  
   - Extrae la `category` (nombre de la categorÃ­a) y `url` (direcciÃ³n web donde se encuentran las imÃ¡genes).  

4. **Llama a la funciÃ³n de descarga de imÃ¡genes** ğŸ“¸  
   - `self.download_products_images(url, category)` procesa la pÃ¡gina y **descarga las imÃ¡genes correspondientes a cada categorÃ­a**.  

---

# FOTO 

## ğŸ–¥ï¸ Bloque `if __name__ == "__main__"`

Este bloque de cÃ³digo **ejecuta el programa en local** y **pone en marcha el scraper** ğŸğŸš€.

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**

1. **Verifica que el script se estÃ¡ ejecutando directamente** â–¶ï¸  
   - La lÃ­nea `if __name__ == '__main__':` **asegura que el cÃ³digo solo se ejecute si este archivo es ejecutado directamente**, y **no si es importado como mÃ³dulo en otro script**.  

2. **Crea una instancia del scraper** ğŸ—ï¸  
   - `scrapper = PCscrapper()` inicializa un objeto de la clase `PCscrapper`, que contiene toda la lÃ³gica del scraping.  
   - AquÃ­ se configuran las opciones y los parÃ¡metros del scraper.  

3. **Ejecuta el scraper** ğŸ”„  
   - `scrapper.run()` llama a la funciÃ³n `run()` para iniciar la **descarga y organizaciÃ³n de imÃ¡genes** desde las pÃ¡ginas web.  
   - Esto hace que **se ejecuten todos los procesos del scraper automÃ¡ticamente**.  

---

# ğŸ–¥ï¸ **Scraping de datos de ratones en PCComponentes para Power BI** ğŸ“ŠğŸ­  

Para la **visualizaciÃ³n de datos en Power BI**, pensÃ© que ya tenÃ­a las imÃ¡genes, asÃ­ queâ€¦ **Â¿quÃ© tan difÃ­cil podrÃ­a ser obtener los datos de los ratones ?** ğŸ¤”  

Bueno, resultÃ³ ser **el mayor reto al que me he enfrentado**.  

### ğŸš§ **El desafÃ­o**  
Por mÃ¡s que intentaba localizar los datos, **cambiaba el nombre de las class una y otra vez**, pero nada funcionaba. Cuando estaba **a punto de rendirme**, tomÃ© aire, suspirÃ© y decidÃ­ empezar **desde cero, paso a paso**.  

### ğŸ” **Paso a paso, desentraÃ±ando los datos**  
1ï¸âƒ£ **Extraer el nombre**: Fue lo mÃ¡s fÃ¡cil, asÃ­ que comencÃ© por ahÃ­. âœï¸  
2ï¸âƒ£ **Obtener el precio**: Perfecto, ahora tenÃ­a **2 de los 5 datos** que necesitaba. ğŸ’°  
3ï¸âƒ£ **Conseguir la URL**: Tras algunos intentos, me di cuenta de que todas las URLs seguÃ­an el mismo patrÃ³n:  
   **Dominio de la web + Nombre del producto** ğŸŒğŸ”—  
   Â¡Un descubrimiento clave! De repente, **todas las URLs estaban listas**.  
4ï¸âƒ£ **Valoraciones y cantidad de opiniones**: Ahora que entendÃ­a la estructura, estos datos vinieron **de inmediato**. â­ğŸ’¬  

### ğŸ”„ **El problema de la paginaciÃ³n**  
Cuando intentÃ© pasar a la **siguiente pÃ¡gina**, **el navegador no respondÃ­a correctamente**. ğŸ˜¡  
AsÃ­ que decidÃ­ **cerrarlo y abrirlo en cada nueva pÃ¡gina**. Â¡FuncionÃ³! Ahora tenÃ­a **27 archivos CSV**, uno por cada pÃ¡gina de productos. ğŸ“‚âœ…  

### ğŸ† **Lo que aprendÃ­**  
âœ… A veces, lo mejor es **empezar desde cero y avanzar poco a poco**.  
âœ… **Observar patrones** en los datos puede hacerte la vida mÃ¡s fÃ¡cil.  
âœ… **Cada problema tiene una soluciÃ³n** (aunque implique reiniciar el navegador ğŸ”„).  

PrÃ³ximo paso: **unir los 27 CSV en un solo archivo** y prepararlo para **Power BI**. ğŸš€ğŸ“Š  

---

## CÃ³digo para la obtenciÃ³n de datos. 

# Foto 

## ğŸ“¦ **MÃ³dulos importados en el scraper**  

Este bloque de cÃ³digo **importa todas las librerÃ­as necesarias** para el funcionamiento del scraper. ğŸ“¥ğŸ”  

### ğŸ› ï¸ **Â¿Para quÃ© sirve cada una?**  

#### â³ **Manejo del tiempo**  
- `import time` â†’ Permite **pausar la ejecuciÃ³n** del cÃ³digo en momentos clave.  

#### ğŸ“„ **Manejo de archivos CSV y texto**  
- `import csv` â†’ Para **guardar los datos extraÃ­dos** en archivos CSV.  
- `import re` â†’ **Expresiones regulares**, Ãºtil para limpiar y estructurar texto.  
- `import unidecode` â†’ **Elimina acentos y caracteres especiales**, Ãºtil para URLs.  

#### ğŸ“‚ **Manejo del sistema de archivos**  
- `import os` â†’ Permite **crear directorios y manejar rutas de archivos**.  

#### ğŸŒ **AutomatizaciÃ³n del navegador con Selenium**  
- `from selenium import webdriver` â†’ Para **controlar el navegador web**.  
- `from selenium.webdriver.chrome.service import Service` â†’ Gestiona **el servicio de ChromeDriver**.  
- `from selenium.webdriver.chrome.options import Options` â†’ Configura **opciones avanzadas del navegador**.  
- `from selenium.webdriver.common.by import By` â†’ Facilita **la bÃºsqueda de elementos en la web**.  
- `from selenium.webdriver.support.ui import WebDriverWait` â†’ Permite **esperar a que aparezcan elementos en la pÃ¡gina**.  
- `from selenium.webdriver.support import expected_conditions as EC` â†’ Define **condiciones especÃ­ficas para esperar elementos** (ejemplo: "cuando un botÃ³n sea clickeable").  

---

# Foto 

## ğŸ–¥ï¸ **Clase `PCScraper`**

Esta clase define el **scraper** encargado de extraer informaciÃ³n de ratones en **PCComponentes** ğŸ­ğŸ›’.

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**

1. **Define el `chromedriver.exe` como controlador del navegador** ğŸï¸  
   - El parÃ¡metro `driver_path="chromedriver.exe"` establece **la ruta del driver de Chrome** para automatizar la navegaciÃ³n.  
   - Esto permite que Selenium **controle el navegador y acceda a las pÃ¡ginas de productos**.  

2. **Genera un diccionario con todas las URLs de las pÃ¡ginas** ğŸŒ  
   - `self.category_urls` almacena un diccionario donde:  
     - La **clave** es el identificador de la pÃ¡gina (`ratones_p1`, `ratones_p2`, etc.).  
     - El **valor** es la **URL completa** de cada pÃ¡gina de productos.  
   - Usa **un bucle con `range(1, 29)`**, generando automÃ¡ticamente **las 28 pÃ¡ginas** de resultados.  

3. **Guarda la ruta del controlador y la URL base** ğŸ”—  
   - `self.driver_path = driver_path` almacena la ubicaciÃ³n del **ChromeDriver**.  
   - `self.base_url = "https://www.pccomponentes.com/"` establece la **URL base de la tienda**.  

---

# Foto 

## ğŸŒ **FunciÃ³n `start_driver`**

Esta funciÃ³n **inicia una nueva sesiÃ³n del navegador Chrome** con configuraciones personalizadas para **optimizar el scraping** y **evitar bloqueos**. ğŸï¸ğŸ’¨  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Crea una configuraciÃ³n especial para el navegador** âš™ï¸  
   - `chrome_options = Options()` inicializa un objeto para **definir opciones avanzadas de Chrome**.  
   - `chrome_options.add_argument("--start-maximized")` hace que el navegador **se inicie en pantalla completa**, lo que **reduce errores de carga y mejora la visibilidad de los elementos**.  

2ï¸âƒ£ **Evita la detecciÃ³n como bot** ğŸ•µï¸â€â™‚ï¸  
   - `chrome_options.add_argument("user-agent=Mozilla/...")` cambia el **User-Agent** del navegador.  
   - Simula que la peticiÃ³n proviene de un usuario real en lugar de un bot, lo que **disminuye la probabilidad de ser bloqueado** por la web.  

3ï¸âƒ£ **Inicia el navegador con la configuraciÃ³n establecida** ğŸš€  
   - `self.driver = webdriver.Chrome(service=Service(self.driver_path), options=chrome_options)`  
   - **Se usa `Service(self.driver_path)`** para asegurarse de que el controlador **ChromeDriver** funcione correctamente.  
   - Todas las opciones configuradas se aplican para garantizar **una navegaciÃ³n fluida y sin detecciÃ³n**.  

---

# FOTO 

## âŒ **FunciÃ³n `close_driver`**  

Esta funciÃ³n **cierra el navegador** una vez que el scraping ha terminado, asegurando que los recursos del sistema se liberen correctamente. ğŸŒğŸ”š  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Cierra la sesiÃ³n del navegador** ğŸ  
   - `self.driver.quit()` **cierra completamente la ventana de Chrome**, finalizando la sesiÃ³n de Selenium.  
   - Esto **libera memoria y evita que queden procesos abiertos** innecesariamente.  

---

# FOTO 

## ğŸª **FunciÃ³n `accept_cookies`**  

Esta funciÃ³n **acepta las cookies automÃ¡ticamente** si el botÃ³n estÃ¡ presente en la pÃ¡gina, evitando interrupciones durante el scraping. ğŸŒğŸ”˜  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Espera a que el botÃ³n de aceptar cookies estÃ© disponible** â³  
   - Usa `WebDriverWait(self.driver, 5).until(EC.element_to_be_clickable((By.ID, 'cookiesAcceptAll')))` para **esperar hasta 5 segundos** a que el botÃ³n sea clickeable.  

2ï¸âƒ£ **Hace clic en el botÃ³n de aceptaciÃ³n** ğŸ‘†  
   - Si el botÃ³n se encuentra, `accept_button.click()` lo presiona automÃ¡ticamente.  
   - `time.sleep(2)` agrega una pequeÃ±a pausa de **2 segundos** para asegurar que el clic se registre correctamente.  

3ï¸âƒ£ **Manejo de errores** âš ï¸  
   - Si el botÃ³n **no aparece o las cookies ya fueron aceptadas**, **se captura la excepciÃ³n**.  
   - Muestra el mensaje `"ğŸ”¹ No se encontrÃ³ el botÃ³n de cookies o ya fueron aceptadas."`, permitiendo que el cÃ³digo **siga ejecutÃ¡ndose sin problemas**.  

---

# Foto 

## ğŸ”— **FunciÃ³n `generate_url`**  

Esta funciÃ³n **genera una URL vÃ¡lida** para un producto a partir de su nombre, asegurÃ¡ndose de que tenga el formato adecuado para ser usada en la web. ğŸŒğŸ”„  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Elimina caracteres especiales y acentos** âœ‚ï¸  
   - `unidecode.unidecode(product_name)` convierte caracteres con acentos (ej. `CÃ¡mara`) a su equivalente sin acento (`Camara`).  
   - Esto **garantiza compatibilidad con URLs**, evitando caracteres invÃ¡lidos.  

2ï¸âƒ£ **Filtra caracteres no deseados** ğŸ”  
   - `re.sub(r'[^a-zA-Z0-9\s]', '', clean_name)` elimina **todo lo que no sea letras, nÃºmeros o espacios**.  
   - AsÃ­ se evita que caracteres especiales rompan la URL.  

3ï¸âƒ£ **Convierte el texto a minÃºsculas y reemplaza espacios** ğŸ”¤  
   - `clean_name.lower()` transforma el texto a **minÃºsculas** para mantener consistencia.  
   - `.replace(" ", "-")` reemplaza los espacios por guiones (`-`), que son estÃ¡ndar en URLs.  

4ï¸âƒ£ **Genera la URL final** ğŸ”—  
   - `return f"{self.base_url}{clean_name}"` une la **URL base** con el nombre limpio del producto.  
   - Por ejemplo, si `product_name = "RatÃ³n Ã“ptico Gamer"`, la salida serÃ¡:  
     ```
     https://www.pccomponentes.com/raton-optico-gamer
     ```  
--- 

# FOTO 

## ğŸ›’ **FunciÃ³n `scrape_category`**  

Esta funciÃ³n **extrae los datos de una sola pÃ¡gina de una categorÃ­a especÃ­fica** dentro de la tienda online. ğŸ“¦ğŸ”  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Inicia el navegador** ğŸŒ  
   - `self.start_driver()` abre una nueva ventana de Chrome para realizar el scraping.  
   - Se asegura de que el navegador **estÃ© configurado correctamente** antes de comenzar.  

2ï¸âƒ£ **Carga la pÃ¡gina de la categorÃ­a** ğŸ”—  
   - `self.driver.get(category_url)` accede a la URL de la categorÃ­a que se quiere analizar.  
   - AquÃ­ es donde estarÃ¡n **todos los productos de esa categorÃ­a**.  

3ï¸âƒ£ **Acepta las cookies si es necesario** ğŸª  
   - `self.accept_cookies()` busca y **hace clic en el botÃ³n de aceptar cookies** si aparece.  
   - Evita que este aviso bloquee la ejecuciÃ³n del scraper.  

### â³ **Esperando la carga de productos en `scrape_category`**  

Esta parte de la funciÃ³n **espera a que los productos de la categorÃ­a se carguen completamente** antes de extraerlos. ğŸ“ŠğŸ•µï¸â€â™‚ï¸  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Espera a que aparezcan los productos** â³  
   - `WebDriverWait(self.driver, 15).until(EC.presence_of_element_located(...))`  
   - **Se espera hasta 15 segundos** para que al menos un producto cargue correctamente.  
   - Busca un `h3.product-card__title`, que es el **tÃ­tulo del producto** en la pÃ¡gina.  

2ï¸âƒ£ **AÃ±ade una pausa extra** âŒ›  
   - `time.sleep(5)` da **5 segundos adicionales** para asegurarse de que **todos los elementos de la pÃ¡gina estÃ©n listos**.  
   - Esto ayuda a evitar errores si el contenido se carga lentamente.  

3ï¸âƒ£ **Extrae la lista de productos** ğŸ›ï¸  
   - `products = self.driver.find_elements(By.CSS_SELECTOR, "div.product-card")`  
   - Encuentra **todos los productos** en la pÃ¡gina, cada uno dentro de un `div.product-card`.  
   - Guarda estos elementos en la lista `products`.  

4ï¸âƒ£ **Prepara la lista de datos** ğŸ“‹  
   - `product_data = []` crea una lista vacÃ­a donde **se guardarÃ¡ la informaciÃ³n de cada producto**.  
   - En los siguientes pasos, se extraerÃ¡n detalles como **nombre, precio, valoraciones, etc.** y se guardarÃ¡n aquÃ­.  

### ğŸ›ï¸ **ExtracciÃ³n de datos de los productos**  

Esta parte de la funciÃ³n **recorre todos los productos de la categorÃ­a** y extrae informaciÃ³n clave como el **nombre, URL y precio**. ğŸ·ï¸ğŸ’°  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Recorre todos los productos encontrados** ğŸ”„  
   - `for product in products:` itera sobre **cada producto en la lista `products`**.  
   - Cada **`product`** representa un artÃ­culo en la pÃ¡gina.  

2ï¸âƒ£ **Extrae el nombre del producto** ğŸ“  
   - `name = product.find_element(By.CSS_SELECTOR, "h3.product-card__title").text.strip()`  
   - Busca el **tÃ­tulo del producto** dentro de la tarjeta (`product-card`).  
   - Usa `.strip()` para **eliminar espacios extra** antes y despuÃ©s del texto.  
   - Si el tÃ­tulo **no estÃ¡ disponible**, asigna `"No disponible"`.  

3ï¸âƒ£ **Genera la URL del producto automÃ¡ticamente** ğŸ”—  
   - `url = self.generate_url(name)`  
   - Llama a la funciÃ³n `generate_url(name)` para **crear una URL vÃ¡lida basada en el nombre**.  
   - Reemplaza espacios y caracteres especiales para que la URL sea compatible con la web.  

4ï¸âƒ£ **Extrae el precio del producto** ğŸ’²  
   - `price = product.find_element(By.CSS_SELECTOR, "span[data-e2e='price-card']").text.strip()`  
   - Busca el precio dentro de un `span` con el atributo `data-e2e='price-card'`.  
   - Si el precio **no estÃ¡ disponible**, asigna `"No disponible"`. 

### â­ **ExtracciÃ³n de Valoraciones y Opiniones**  

Esta parte de la funciÃ³n **extrae la valoraciÃ³n (rating) y el nÃºmero de opiniones** de cada producto, asegurÃ¡ndose de que la informaciÃ³n se capture correctamente. ğŸ“ŠğŸ”  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Inicializa valores predeterminados** ğŸ”„  
   - `rating = "No disponible"` y `opinions = "0"` aseguran que si no se encuentra informaciÃ³n, **no haya valores vacÃ­os**.  

2ï¸âƒ£ **Verifica si el producto tiene valoraciones** â­  
   - `rating_container = product.find_element(By.CSS_SELECTOR, ".product-card__rating-container")`  
   - Busca el **contenedor de valoraciones** dentro de la tarjeta del producto.  
   - Si no existe, el cÃ³digo **continÃºa sin errores**.  

3ï¸âƒ£ **Extrae la valoraciÃ³n numÃ©rica del producto** ğŸ”¢  
   - Se buscan `span` dentro del `rating_container`.  
   - Se analiza cada `span.text` para detectar patrones como `4.5/5` o `4,5/5`.  
   - Usa `re.search(r'[0-9][.,][0-9]/[0-9]', text)` para identificar formatos vÃ¡lidos.  
   - Si se encuentra, se guarda en `rating` y se detiene la bÃºsqueda.  

4ï¸âƒ£ **Extrae el nÃºmero de opiniones** ğŸ’¬  
   - Busca palabras clave como `"opiniÃ³n"` dentro del contenedor de valoraciones.  
   - Extrae el primer nÃºmero encontrado (`\d+`) y lo asigna a `opinions`.  
   - Si no hay opiniones visibles, el valor **permanece en "0"**.  

5ï¸âƒ£ **Manejo de errores** âš ï¸  
   - Si el contenedor de valoraciones **no existe**, el cÃ³digo **sigue sin fallar**.  
   - Se usa `try-except` en varios niveles para **evitar que errores en una parte bloqueen la ejecuciÃ³n completa**.  

### ğŸ“‹ **Almacenamiento de Datos y Manejo de Errores**  

Esta Ãºltima parte de la funciÃ³n **guarda los datos extraÃ­dos en una lista y maneja posibles errores**, asegurando que el scraping se complete correctamente. ğŸ“Šâœ…  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Guarda los datos en una lista** ğŸ“‚  
   - `product_data.append([name, price, rating, opinions, url])`  
   - **Cada producto** se almacena como una lista con los siguientes valores:  
     - ğŸ“Œ `name` â†’ Nombre del producto.  
     - ğŸ’² `price` â†’ Precio actual.  
     - â­ `rating` â†’ ValoraciÃ³n promedio.  
     - ğŸ’¬ `opinions` â†’ NÃºmero de opiniones.  
     - ğŸ”— `url` â†’ Enlace al producto.  
   - Esto permite que todos los datos queden organizados y listos para su uso posterior (ej. guardado en CSV o base de datos).  

2ï¸âƒ£ **Manejo de errores en productos individuales** âš   
   - `except Exception as e:` captura cualquier **error durante la extracciÃ³n de un producto**.  
   - Si un producto falla, muestra el mensaje `âš  Error extrayendo datos de un producto: {e}`.  
   - **El resto de los productos se siguen procesando sin interrupciones**.  

3ï¸âƒ£ **Devuelve la lista de productos extraÃ­dos** âœ…  
   - `return product_data` devuelve **todos los datos recopilados** en la categorÃ­a actual.  
   - Si no se encuentran productos, se devuelve una **lista vacÃ­a** `[]`.  

4ï¸âƒ£ **Manejo de errores en toda la categorÃ­a** âŒ  
   - Si hay un error **en toda la pÃ¡gina**, lo captura `except Exception as e:`.  
   - Muestra `âŒ Error al obtener los productos de {category_url}: {e}` para indicar quÃ© fallÃ³.  
   - En este caso, tambiÃ©n devuelve `[]`, asegurando que el scraper no se detenga completamente.  

5ï¸âƒ£ **Cierra el navegador al finalizar** ğŸŒğŸ”š  
   - `finally: self.close_driver()`  
   - **Se cierra el navegador en todos los casos**, ya sea que la extracciÃ³n haya sido exitosa o haya ocurrido un error.  
   - Esto evita **consumo innecesario de recursos** y posibles bloqueos en futuras ejecuciones.  

---

# Foto 

### ğŸ“‚ **FunciÃ³n `scrape_all_categories`**  

Esta funciÃ³n **recorre todas las pÃ¡ginas de productos de la categorÃ­a y guarda los datos en archivos CSV separados**. ğŸ“ŠğŸ›ï¸  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Recorre todas las categorÃ­as disponibles** ğŸ”„  
   - `for category_name, category_url in self.category_urls.items():`  
   - Itera sobre el **diccionario `self.category_urls`**, que contiene los nombres y URLs de cada categorÃ­a.  

2ï¸âƒ£ **Llama a la funciÃ³n `scrape_category` para extraer los productos** ğŸ›’  
   - `product_data = self.scrape_category(category_name, category_url)`  
   - Llama a la funciÃ³n `scrape_category` para **extraer los productos de la pÃ¡gina actual**.  
   - Si la categorÃ­a **tiene productos**, se procede a guardarlos.  

3ï¸âƒ£ **Guarda los datos en un archivo CSV** ğŸ’¾  
   - **Genera un nombre de archivo** basado en la categorÃ­a:  
     ```python
     csv_filename = f"{category_name}.csv"
     ```
   - **Abre el archivo CSV en modo escritura (`"w"`)**, asegurando que se cree desde cero.  
   - **Escribe los encabezados** en la primera fila:  
     - ğŸ“Œ `Nombre del Producto`  
     - ğŸ’² `Precio`  
     - â­ `Valoraciones`  
     - ğŸ’¬ `Opiniones`  
     - ğŸ”— `URL`  
   - **Escribe los datos de los productos** fila por fila con `writer.writerows(product_data)`.  

4ï¸âƒ£ **Muestra un mensaje de Ã©xito** âœ…  
   - Indica cuÃ¡ntos productos se han guardado y en quÃ© archivo CSV.  
   - Ejemplo de salida:  
     ```
     âœ… Se han guardado 50 productos en 'ratones_p1.csv'.
     ```

5ï¸âƒ£ **Espera 5 segundos antes de continuar con la siguiente pÃ¡gina** â³  
   - `time.sleep(5)`  
   - **Evita que el scraper sea detectado como bot**, simulando un comportamiento humano.  

---

# Foto 

## ğŸ **EjecuciÃ³n del Scraper**  

Este bloque de cÃ³digo **pone en marcha el scraper**, iniciando el proceso de extracciÃ³n de datos de todas las categorÃ­as. ğŸš€ğŸ“Š  

### ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Verifica que el script se estÃ¡ ejecutando directamente** â–¶ï¸  
   - `if __name__ == "__main__":`  
   - Este condicional asegura que el cÃ³digo **solo se ejecute si el script es ejecutado directamente**, y **no si es importado como mÃ³dulo** en otro archivo.  

2ï¸âƒ£ **Crea una instancia del scraper** ğŸ—ï¸  
   - `scraper = PCScraper()`  
   - Se inicializa un objeto de la clase `PCScraper`, que contiene toda la lÃ³gica de scraping.  
   - AquÃ­ se configuran **las URLs de las categorÃ­as y las opciones del navegador**.  

3ï¸âƒ£ **Ejecuta el scraping de todas las categorÃ­as** ğŸ”„  
   - `scraper.scrape_all_categories()`  
   - Llama a la funciÃ³n que **recorre todas las pÃ¡ginas de productos y guarda los datos en archivos CSV**.  
   - **Cada categorÃ­a se procesa de manera independiente**, asegurando que todas las pÃ¡ginas sean analizadas.  

---

# ğŸ¯ **ConclusiÃ³n del Apartado de ObtenciÃ³n de Datos**  

Hemos completado con Ã©xito el proceso de **extracciÃ³n de datos** con un scraper totalmente funcional. ğŸš€ğŸ“Š  

## âœ… **Â¿QuÃ© hemos logrado?**  

1ï¸âƒ£ **AutomatizaciÃ³n completa** ğŸ”„  
   - Desde **abrir el navegador** hasta **guardar los datos en archivos CSV**.  

2ï¸âƒ£ **ExtracciÃ³n eficiente** ğŸ“Š  
   - ObtenciÃ³n de **nombre del producto, precio, valoraciones, opiniones y URL**.  

3ï¸âƒ£ **Manejo de errores inteligente** âš ï¸  
   - **El scraper sigue funcionando** incluso si algunos datos no estÃ¡n disponibles.  

4ï¸âƒ£ **OptimizaciÃ³n anti-detecciÃ³n** ğŸ•µï¸â€â™‚ï¸  
   - **User-Agent personalizado** para evitar bloqueos.  
   - **Pausas estratÃ©gicas** para simular un usuario real.  

5ï¸âƒ£ **Datos listos para anÃ¡lisis** ğŸ“‚  
   - Todos los productos se han almacenado en **archivos CSV organizados**.  
   - **Perfecto para su visualizaciÃ³n en Power BI u otras herramientas de anÃ¡lisis**.  

---

## 3. Limpieza de datos (eliminaciÃ³n de nulos y datos errÃ³neos, etc.). DescripciÃ³n de los datos. Se debe dar una descripciÃ³n completa de los datos indicando quÃ© significa cada uno de los atributos.

# ğŸ§¹ **Limpieza de Datos**  

El proceso de limpieza de datos en este proyecto es **sencillo pero esencial** para garantizar que la informaciÃ³n extraÃ­da sea **relevante y precisa**. ğŸš€ğŸ“Š  

---

## ğŸ¯ **Â¿QuÃ© limpiaremos?**  

ğŸ”¹ **ImÃ¡genes duplicadas y no relevantes** ğŸ–¼ï¸  
   - Se han detectado **imÃ¡genes promocionales** que **se repiten en el mismo orden** para ratones, teclados y portÃ¡tiles.  
   - Estas imÃ¡genes **usan la misma clase (`class`) que las imÃ¡genes de productos**, por lo que se han incluido accidentalmente en la extracciÃ³n.  

ğŸ”¹ **Filtrado de imÃ¡genes segÃºn patrones** ğŸ”  
   - Dado que **las imÃ¡genes promocionales siguen un patrÃ³n especÃ­fico**, se pueden **identificar y eliminar fÃ¡cilmente**.  
   - Aunque el cÃ³digo varÃ­a ligeramente entre categorÃ­as (**ratones, teclados, portÃ¡tiles**), la lÃ³gica **es la misma** en todos los casos.  

---

## ğŸ› ï¸ **Â¿CÃ³mo se hace la limpieza?**  

1ï¸âƒ£ **IdentificaciÃ³n de imÃ¡genes no deseadas**  
   - Se analizan **los patrones de repeticiÃ³n** en las imÃ¡genes extraÃ­das.  
   - Se detectan aquellas que corresponden a **banners publicitarios o contenido promocional**.  

2ï¸âƒ£ **Filtrado automÃ¡tico en el cÃ³digo**  
   - Se implementa una lÃ³gica que **excluye las imÃ¡genes** basÃ¡ndose en su posiciÃ³n o en atributos especÃ­ficos.  
   - Si una imagen **coincide con el patrÃ³n promocional**, **se descarta antes de almacenarla**.  

3ï¸âƒ£ **Mantenimiento de la calidad de datos**  
   - Solo se conservan **imÃ¡genes relevantes de los productos reales**.  
   - Se evita la presencia de contenido duplicado en la base de datos.  

---

## ğŸ”¥ **Â¿Por quÃ© es importante este proceso?**  

âœ… **Elimina imÃ¡genes irrelevantes**, manteniendo el dataset limpio.  
âœ… **Evita duplicados**, mejorando la calidad de los datos.    

---

# Foto 

# ğŸ—‘ï¸ **EliminaciÃ³n de ImÃ¡genes No Deseadas**  

Este cÃ³digo **elimina imÃ¡genes repetidas o no relevantes** dentro de la carpeta de portÃ¡tiles, asegurando que solo se mantengan las imÃ¡genes Ãºtiles. ğŸ–¼ï¸ğŸš®  

---

## ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Define la carpeta principal** ğŸ“‚  
   - `main_folder = "imagenes_portatiles"`  
   - AquÃ­ es donde se encuentran las subcarpetas con las imÃ¡genes almacenadas.  

2ï¸âƒ£ **Lista de archivos a eliminar** âŒ  
   - `files_to_delete = {"image"}`  
   - Se crea un **conjunto** con los nombres de archivos que deben ser eliminados.  
   - En este caso, cualquier archivo llamado `"image"` serÃ¡ **borrado automÃ¡ticamente**.  

3ï¸âƒ£ **Recorre todas las subcarpetas** ğŸ”„  
   - Usa `os.walk(main_folder)` para **iterar sobre cada subcarpeta** dentro de `imagenes_portatiles`.  
   - En cada subcarpeta, **verifica los archivos almacenados**.  

4ï¸âƒ£ **Elimina archivos innecesarios** ğŸ—‘ï¸  
   - Si un archivo coincide con un nombre en `files_to_delete`, se borra usando `os.remove(file_path)`.  
   - Se muestra un mensaje confirmando cada eliminaciÃ³n con `print(f"Eliminado: {file_path}")`.  

5ï¸âƒ£ **Manejo de errores** âš ï¸  
   - Si ocurre un error al eliminar un archivo, se captura y muestra un mensaje de advertencia:  
     ```
     Error al eliminar <ruta_del_archivo>: <detalle_del_error>
     ```  
   - Esto evita que el cÃ³digo se detenga si hay problemas con permisos o archivos inexistentes.  

6ï¸âƒ£ **Finaliza el proceso** âœ…  
   - Al terminar, imprime `"Proceso completado."` indicando que la limpieza ha sido exitosa.  


ğŸ“Œ **Con esta limpieza, garantizamos que los datos extraÃ­dos sean Ãºtiles y sin ruido.**  

## 4. ExploraciÃ³n y visualizaciÃ³n de los datos. Se realizarÃ¡ un estudio de los datos buscando correlaciones, mostrando grÃ¡ficas de diferente tipologÃ­a, observando si hay valores nulos, etc.


# FOto

# ğŸ–¼ï¸ **Almacenamiento de ImÃ¡genes en CSV**  

DespuÃ©s de eliminar las imÃ¡genes no deseadas, el siguiente paso es **registrarlas en un archivo CSV**. ğŸ“‚ğŸ”„  

## ğŸ¯ **Â¿Por quÃ© guardar las imÃ¡genes en un CSV?**  

ğŸ”¹ **OrganizaciÃ³n** â†’ Permite estructurar los datos para su fÃ¡cil anÃ¡lisis.  
ğŸ”¹ **IntegraciÃ³n con Power BI** â†’ Facilita la vinculaciÃ³n con otros datos, como precios o valoraciones.  
ğŸ”¹ **Accesibilidad** â†’ Un CSV es ligero y compatible con mÃºltiples herramientas de anÃ¡lisis.  

---

# Foto 

# ğŸ“‚ **DefiniciÃ³n de Carpetas y Lista de Datos**  

Antes de procesar las imÃ¡genes, es necesario **definir las carpetas de origen** y **crear una lista para almacenar la informaciÃ³n** extraÃ­da. ğŸ–¼ï¸ğŸ“Š  

---

## ğŸ› ï¸ **Â¿CÃ³mo funciona esta parte del cÃ³digo?**  

1ï¸âƒ£ **Definir las carpetas donde estÃ¡n las imÃ¡genes** ğŸ“‚  
   - `carpetas_principales = ["../../ikea_muebles/sillas"]`  
   - Se establece una lista con **las rutas de las carpetas principales**, donde se encuentran las imÃ¡genes organizadas en subcarpetas.  
   - En este caso, se estÃ¡ procesando la carpeta `"sillas"` dentro de `"ikea_muebles"`.  

2ï¸âƒ£ **Crear una lista para almacenar los datos** ğŸ“  
   - `data = []`  
   - Se inicializa una **lista vacÃ­a** que **almacenarÃ¡ la informaciÃ³n de cada imagen**.  
   - Posteriormente, en esta lista se guardarÃ¡n datos como:  
     - ğŸ–¼ï¸ `Nombre del archivo`  
     - ğŸ“‚ `Ruta de la imagen`  
     - ğŸ”  `Imagen codificada en base64`  

---

# Foto

# ğŸ–¼ï¸ **FunciÃ³n `procesar_carpeta`**  

Esta funciÃ³n **recorre carpetas y subcarpetas**, buscando imÃ¡genes, **convirtiÃ©ndolas a Base64** y almacenÃ¡ndolas en una lista con formato HTML. ğŸ“‚ğŸ“Š  

---

## ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Recorre todas las carpetas y subcarpetas** ğŸ”„  
   - `os.walk(os.path.abspath(carpeta_raiz))`  
   - Convierte la **ruta relativa en absoluta** para evitar errores.  
   - **Recorre recursivamente** todas las carpetas y subcarpetas dentro de `carpeta_raiz`.  

2ï¸âƒ£ **Filtra archivos de imagen** ğŸ·ï¸  
   - `if archivo.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):`  
   - **Solo procesa archivos de imagen** con extensiones comunes.  
   - Ignora otros tipos de archivos no relevantes.  

3ï¸âƒ£ **Convierte la imagen a Base64** ğŸ”„  
   - `with open(ruta_imagen, "rb") as img_file:`  
   - Abre la imagen en **modo lectura binaria (`rb`)**.  
   - `base64.b64encode(img_file.read()).decode('utf-8')`  
   - **Codifica la imagen en Base64** y la convierte en un **string de texto**.  

4ï¸âƒ£ **Genera una etiqueta HTML con la imagen en Base64** ğŸ–¼ï¸  
   - `img_html = f'<img src="data:image/png;base64,{base64_str}" width="100"/>'`  
   - **Crea un fragmento HTML** que puede ser interpretado directamente en Power BI u otras herramientas.  
   - Se establece un **ancho de `100px`** para previsualizaciÃ³n.  

5ï¸âƒ£ **Agrega la imagen a la lista de datos** ğŸ“‹  
   - `data.append([img_html])`  
   - Guarda la informaciÃ³n en la lista `data`, para su posterior almacenamiento en CSV.  

6ï¸âƒ£ **Manejo de errores** âš ï¸  
   - Si ocurre algÃºn problema al procesar una imagen, el error **se muestra en consola** y el programa sigue ejecutÃ¡ndose.  

---

# Foto 

# ğŸ“„ **ConversiÃ³n de ImÃ¡genes a CSV**  

DespuÃ©s de procesar todas las imÃ¡genes, este cÃ³digo **crea un DataFrame y lo guarda en un archivo CSV**, asegurando que estÃ© listo para su uso en Power BI u otras herramientas. ğŸ“‚ğŸ“Š  

---

## ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Procesa todas las carpetas principales** ğŸ“‚  
   - `for carpeta in carpetas_principales:`  
   - **Recorre cada carpeta** y ejecuta `procesar_carpeta(carpeta)`.  
   - Se almacenan las imÃ¡genes **convertidas a Base64 con formato HTML** en la lista `data`.  

2ï¸âƒ£ **Crea un DataFrame con los datos** ğŸ—ï¸  
   - `df = pd.DataFrame(data, columns=["chair"])`  
   - Se genera un **DataFrame de Pandas** con una columna llamada `"chair"`.  
   - **Cada fila contiene una imagen en formato Base64 con etiqueta HTML**.  

3ï¸âƒ£ **Guarda el DataFrame en un archivo CSV** ğŸ’¾  
   - `csv_path = "chair.csv"` define el nombre del archivo.  
   - `df.to_csv(csv_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL, escapechar="\\")`  
     - ğŸ”¹ **`index=False`** â†’ No guarda el Ã­ndice del DataFrame.  
     - ğŸ”¹ **`encoding="utf-8-sig"`** â†’ Asegura compatibilidad con **Power BI y Excel**.  
     - ğŸ”¹ **`quoting=csv.QUOTE_MINIMAL`** â†’ Evita problemas con comillas en los datos.  
     - ğŸ”¹ **`escapechar="\\ "`** â†’ Escapa caracteres especiales para evitar errores en la lectura del CSV.  

4ï¸âƒ£ **Muestra un mensaje de Ã©xito** âœ…  
   - `print(f"âœ… Archivo CSV guardado correctamente en: {csv_path}")`  
   - Confirma que el archivo se ha guardado **sin errores y listo para su anÃ¡lisis**.  

---

# Foto 

# ğŸ“¦ **ImportaciÃ³n de MÃ³dulos para la Prueba y VisualizaciÃ³n**  

Antes de verificar el correcto funcionamiento de los datos, necesitamos **importar las librerÃ­as necesarias** para **cargar, procesar y visualizar las imÃ¡genes** almacenadas en el CSV. ğŸ“ŠğŸ–¼ï¸  

---

## ğŸ› ï¸ **Â¿QuÃ© hace cada mÃ³dulo?**  

1ï¸âƒ£ **`pandas` â†’ Manejo de Datos Tabulares** ğŸ“Š  
   - `import pandas as pd`  
   - Permite **cargar el CSV** y trabajar con Ã©l como un **DataFrame**.  
   - Se usarÃ¡ para leer y verificar la estructura de los datos.  

2ï¸âƒ£ **`matplotlib.pyplot` â†’ VisualizaciÃ³n de Datos** ğŸ“ˆ  
   - `import matplotlib.pyplot as plt`  
   - Nos permite **mostrar las imÃ¡genes** contenidas en el CSV.  
   - Se usarÃ¡ para graficar y confirmar que las imÃ¡genes se han guardado correctamente.  

3ï¸âƒ£ **`base64` â†’ DecodificaciÃ³n de ImÃ¡genes** ğŸ”„  
   - `import base64`  
   - Convierte las imÃ¡genes **de Base64 a un formato visualizable**.  
   - Se usarÃ¡ para reconstruir las imÃ¡genes almacenadas en el CSV.  

4ï¸âƒ£ **`BytesIO` â†’ Manejo de Archivos en Memoria** ğŸ’¾  
   - `from io import BytesIO`  
   - Permite trabajar con **imÃ¡genes sin necesidad de guardarlas en disco**.  
   - Se usarÃ¡ para cargar imÃ¡genes directamente en memoria antes de visualizarlas.  

5ï¸âƒ£ **`PIL.Image` â†’ Procesamiento de ImÃ¡genes** ğŸ–¼ï¸  
   - `from PIL import Image`  
   - Nos permite **abrir, procesar y mostrar imÃ¡genes** en Python.  
   - Se usarÃ¡ para reconstruir las imÃ¡genes en Base64 y mostrarlas en pantalla.  

---

# Foto 

# ğŸ“„ **Lectura y ExtracciÃ³n de ImÃ¡genes desde CSV**  

En este paso, **cargamos el archivo CSV y extraemos la primera imagen** almacenada en formato Base64 para verificar su correcta codificaciÃ³n. ğŸ“‚ğŸ–¼ï¸  

---

## ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Carga el archivo CSV** ğŸ“„  
   - `df = pd.read_csv('chair.csv')`  
   - **Lee el archivo `chair.csv`** usando Pandas y lo almacena en un **DataFrame**.  
   - Contiene la columna `"chair"` con imÃ¡genes en formato **Base64 dentro de etiquetas HTML**.  

2ï¸âƒ£ **Obtiene la primera fila del DataFrame** ğŸ”  
   - `first_row = df.iloc[0]`  
   - Usa `.iloc[0]` para **seleccionar la primera fila** del DataFrame.  
   - Se extrae una imagen **para verificar que los datos estÃ¡n bien guardados**.  

3ï¸âƒ£ **Extrae la imagen en Base64** ğŸ—ï¸  
   - `img_base64 = first_row['chair']`  
   - Se accede a la columna `"chair"` de la primera fila.  
   - **AquÃ­ se encuentra el cÃ³digo Base64 dentro de una etiqueta HTML**.  

---

# Foto 

# ğŸ–¼ï¸ **VerificaciÃ³n, DecodificaciÃ³n y VisualizaciÃ³n de la Imagen**  

Este cÃ³digo **verifica que la imagen en Base64 estÃ© en el formato correcto, la decodifica y la muestra en pantalla**. ğŸ“‚ğŸ”  

---

## ğŸ› ï¸ **Â¿CÃ³mo funciona?**  

1ï¸âƒ£ **Verifica si la cadena Base64 tiene un prefijo de datos** ğŸ”  
   - `if ',' in img_base64:`  
   - Algunas imÃ¡genes en Base64 **pueden incluir un prefijo**, por ejemplo:  
     ```
     data:image/png;base64,iVBORw...
     ```
   - Si hay una coma `,`, significa que el prefijo estÃ¡ presente.  
   - `img_base64.split(',')[1]` extrae **solo la parte Base64**, eliminando `"data:image/png;base64,"`.  

2ï¸âƒ£ **Asegura que la longitud de la cadena sea un mÃºltiplo de 4** ğŸ”¢  
   - `padding = len(img_base64) % 4`  
   - Base64 **debe tener una longitud en mÃºltiplos de 4**.  
   - Si no lo es, **se agregan los caracteres `=` necesarios** para corregir la cadena.  
   - `img_base64 += '=' * (4 - padding)` **corrige la longitud si es necesario**.  

3ï¸âƒ£ **Intenta decodificar la imagen** ğŸ—ï¸  
   - `img_data = base64.b64decode(img_base64)`  
   - Convierte la cadena **de Base64 a datos binarios de imagen**.  

4ï¸âƒ£ **Convierte los datos en una imagen visualizable** ğŸ–¼ï¸  
   - `img = Image.open(BytesIO(img_data))`  
   - Usa `BytesIO` para **convertir los datos binarios en una imagen sin guardarla en disco**.  
   - `Image.open()` abre la imagen lista para ser visualizada.  

5ï¸âƒ£ **Muestra la imagen** ğŸ“Š  
   - `plt.figure()` â†’ Crea una nueva figura para la imagen.  
   - `plt.imshow(img)` â†’ Muestra la imagen decodificada.  
   - `plt.axis('off')` â†’ Oculta los ejes para mejorar la visualizaciÃ³n.  
   - `plt.show()` â†’ Muestra la imagen en pantalla.  

6ï¸âƒ£ **Manejo de errores** âš ï¸  
   - Si ocurre un error en la decodificaciÃ³n, se captura con `except Exception as e`.  
   - Se imprime un mensaje de error con `print(f"Error al decodificar la imagen: {e}")`.  

---

# Foto 

# ğŸ–±ï¸ **VerificaciÃ³n de Datos de Ratones**  

Ahora que hemos extraÃ­do y almacenado los datos, **vamos a verificar su integridad** antes de proceder con la visualizaciÃ³n en Power BI. ğŸ“Š  

---

# Foto

## ğŸ› ï¸ **Â¿CÃ³mo funciona esta prueba?**  

1ï¸âƒ£ **Carga el archivo CSV** ğŸ“‚  
   - `df_datos = pd.read_csv('/content/ratones_p1.csv')`  
   - Usa Pandas para **leer los datos almacenados en el archivo CSV**.  

2ï¸âƒ£ **Muestra las primeras filas** ğŸ”  
   - `print(df_datos.head())`  
   - Permite visualizar las primeras 5 filas del dataset para asegurarnos de que los datos estÃ¡n bien organizados.  

3ï¸âƒ£ **Verifica si hay valores nulos** âš ï¸  
   - `print(df_datos.isna())`  
   - Devuelve un **DataFrame con valores `True` o `False`**, indicando si hay datos faltantes.  

4ï¸âƒ£ **Cuenta los valores nulos por columna** ğŸ”¢  
   - `print(df_datos.isna().sum())`  
   - Muestra **cuÃ¡ntos valores nulos hay en cada columna**, ayudando a identificar posibles problemas en los datos.  

---

## 5. PreparaciÃ³n de los datos para los algoritmos de Machine Learning. Se deben tratar los datos (limpiando, escalando, separando y todo lo que sea necesario) de tal forma que queden listos para entrenar el modelo.

## 6. Entrenamiento del modelo y comprobaciÃ³n del rendimiento. Se entrenarÃ¡n uno o varios modelos, comprobando en cada caso el rendimiento que ofrecen mediante las apropiadas medidas de error y/o acierto.

## 7. Se tiene que incluir alguna de las tÃ©cnicas estudiadas en el tema de Procesamiento de Lenguaje Natural: expresiones regulares, tokenizaciÃ³n, generaciÃ³n de texto, anÃ¡lisis de sentimientos, etc.

## 8. Se debe realizar tambiÃ©n una aplicaciÃ³n web que haga uso del modelo entrenado.

## 9. Conclusiones. Se expondrÃ¡n las conclusiones que se han obtenido en la realizaciÃ³n del TFM.
